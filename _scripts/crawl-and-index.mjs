import fs from 'fs'
import path from 'path'
import { fileURLToPath } from 'url'
import { JSDOM } from 'jsdom'
import { slugifyWithCounter } from '@sindresorhus/slugify'

const __dirname = path.dirname(fileURLToPath(import.meta.url))

// Get all slug paths from MDX files
function getSlugPaths(dir, basePath = '') {
  const out = []
  const entries = fs.readdirSync(dir, { withFileTypes: true })

  for (const entry of entries) {
    const fullPath = path.join(dir, entry.name)
    const relative = basePath ? `${basePath}/${entry.name}` : entry.name

    if (entry.isDirectory()) {
      out.push(...getSlugPaths(fullPath, relative))
      continue
    }

    if (!entry.name.endsWith('.mdx')) continue

    const slugPathNoExt = relative.replace(/\.mdx$/, '')
    const segs = slugPathNoExt.split('/')
    const slugPath = segs.join('/') || 'index'

    out.push(slugPath)
  }

  return out
}

// Fetch HTML from local server
async function fetchPage(url) {
  try {
    const response = await fetch(url)
    if (!response.ok) {
      throw new Error(`HTTP ${response.status}`)
    }
    return await response.text()
  } catch (error) {
    throw new Error(`Failed to fetch ${url}: ${error.message}`)
  }
}

// Extract text content from an element, excluding script/style tags
function getTextContent(element) {
  const clone = element.cloneNode(true)
  // Remove script, style, nav, header, footer, and aside elements
  clone
    .querySelectorAll('script, style, nav, header, footer, aside')
    .forEach((el) => el.remove())
  return clone.textContent.trim().replace(/\s+/g, ' ')
}

// Parse HTML and extract sections based on headings
function extractSectionsFromHtml(html, url) {
  const dom = new JSDOM(html)
  const document = dom.window.document
  const sections = []

  // Focus only on content within main element
  const main = document.querySelector('main')
  if (!main) {
    console.warn(`No <main> element found on ${url}`)
    return sections
  }

  // Remove header, footer, and aside elements from main before processing
  main.querySelectorAll('header, footer, aside').forEach((el) => el.remove())

  // Find all headings (h1-h6) within main
  const headings = main.querySelectorAll('h1, h2, h3, h4, h5, h6')

  let pageTitle =
    main.querySelector('h1')?.textContent.trim() ||
    document.querySelector('title')?.textContent.trim() ||
    url.split('/').pop().replace(/-/g, ' ')

  headings.forEach((heading, index) => {
    const id = heading.id || ''
    const title = heading.textContent.trim()
    const headingUrl = `${url}${id ? `#${id}` : ''}`

    // Collect content between this heading and the next
    let content = []
    let currentElement = heading.nextElementSibling
    const nextHeading = headings[index + 1]

    while (currentElement && currentElement !== nextHeading) {
      // Skip nav, header, footer, aside, script, style
      if (
        !['NAV', 'HEADER', 'FOOTER', 'ASIDE', 'SCRIPT', 'STYLE'].includes(
          currentElement.tagName
        )
      ) {
        const text = getTextContent(currentElement)
        if (text) {
          content.push(text)
        }
      }
      currentElement = currentElement.nextElementSibling
    }

    sections.push({
      url: headingUrl,
      title: title,
      content: content.join('\n'),
      pageTitle: id ? pageTitle : undefined,
    })
  })

  return sections
}

async function crawlAndGenerateIndex() {
  console.log('üîç Crawling local site to generate search index...\n')

  const baseDir = path.resolve(__dirname, '../src/app/markdown')
  const slugPaths = getSlugPaths(baseDir)
  const baseUrl = 'http://localhost:3000'

  console.log(`Found ${slugPaths.length} pages to crawl\n`)

  const allSections = []
  let successCount = 0
  let errorCount = 0

  for (const slugPath of slugPaths) {
    const url = slugPath === 'index' ? '/' : `/${slugPath}`
    const fullUrl = `${baseUrl}${url}`

    try {
      console.log(`Crawling: ${url}`)
      const html = await fetchPage(fullUrl)
      const sections = extractSectionsFromHtml(html, url)
      allSections.push(...sections)
      successCount++
    } catch (error) {
      console.error(`‚úó ${url}: ${error.message}`)
      errorCount++
    }
  }

  // Write to file
  const outputPath = path.resolve(__dirname, '../public/search-index.json')
  fs.writeFileSync(outputPath, JSON.stringify(allSections, null, 2))

  console.log(`\n‚úÖ Search index generated with ${allSections.length} entries`)
  console.log(`üìÑ Saved to: ${outputPath}`)
  console.log(`üìä Successfully crawled ${successCount} pages`)
  if (errorCount > 0) {
    console.log(`‚ùå Failed to crawl ${errorCount} pages`)
  }
}

crawlAndGenerateIndex()
